{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrjyZX2v0w1j"
      },
      "source": [
        "Demo based on https://github.com/thunlp/OpenPrompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpDzqqrt0w1o"
      },
      "source": [
        "To be able to run this demo, first you need to install required dependecies:\n",
        "* torch: ```pip install torch```\n",
        "* openprompt: ```pip install openprompt```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch openprompt"
      ],
      "metadata": {
        "id": "ZYbvpX3G00Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zElas2950w1p"
      },
      "source": [
        "### **Step 1: Define a task**\n",
        "The first step is to determine the current NLP task, think about whatâ€™s your data looks like and what do you want from the data! That is, the essence of this step is to determine the classses and the InputExample of the task. For simplicity, we use Sentiment Analysis as an example. tutorial_task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPyzcFb-0w1p"
      },
      "outputs": [],
      "source": [
        "from openprompt.data_utils import InputExample\n",
        "classes = [ # There are two classes in Sentiment Analysis, one for negative and one for positive\n",
        "    \"negative\",\n",
        "    \"positive\"\n",
        "]\n",
        "dataset = [ # For simplicity, there's only two examples\n",
        "    # text_a is the input text of the data, some other datasets may have multiple input sentences in one example.\n",
        "    InputExample(\n",
        "        guid = 0,\n",
        "        text_a = \"Albert Einstein was one of the greatest intellects of his time.\",\n",
        "    ),\n",
        "    InputExample(\n",
        "        guid = 1,\n",
        "        text_a = \"The film was badly made.\",\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMWNB7dS0w1q"
      },
      "source": [
        "### **Step 2: Define a Pre-trained Language Models (PLMs) as backbone.**\n",
        "Choose a PLM to support your task. Different models have different attributes, we encourge you to use OpenPrompt to explore the potential of various PLMs. OpenPrompt is compatible with models on [huggingface](https://huggingface.co/docs/transformers/index)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNVyoT_80w1r",
        "outputId": "78484393-8c93-4cd8-bc84-82523a498520"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from openprompt.plms import load_plm\n",
        "plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjXUUuxt0w1s"
      },
      "source": [
        "### **Step 3: Define a Template.**\n",
        "A Template is a modifier of the original input text, which is also one of the most important modules in prompt-learning.  We have defined text_a in Step 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o80tG8J40w1s"
      },
      "outputs": [],
      "source": [
        "from openprompt.prompts import ManualTemplate\n",
        "promptTemplate = ManualTemplate(\n",
        "    text = '{\"placeholder\":\"text_a\"} It was {\"mask\"}',\n",
        "    tokenizer = tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tEl8QPG0w1t"
      },
      "source": [
        "### **Step 4: Define a Verbalizer**\n",
        "A Verbalizer is another important (but not necessary) in prompt-learning,which projects the original labels (we have defined them as classes, remember?) to a set of label words. Here is an example that we project the negative class to the word bad, and project the positive class to the words good, wonderful, great."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPuaqWxT0w1u"
      },
      "outputs": [],
      "source": [
        "from openprompt.prompts import ManualVerbalizer\n",
        "promptVerbalizer = ManualVerbalizer(\n",
        "    classes = classes,\n",
        "    label_words = {\n",
        "        \"negative\": [\"bad\"],\n",
        "        \"positive\": [\"good\", \"wonderful\", \"great\"],\n",
        "    },\n",
        "    tokenizer = tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA1RkzFt0w1u"
      },
      "source": [
        "### **Step 5: Combine them into a PromptModel**\n",
        "Given the task, now we have a PLM, a Template and a Verbalizer, we combine them into a PromptModel. Note that although the example naively combine the three modules, you can actually define some complicated interactions among them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_2Px-EY0w1u"
      },
      "outputs": [],
      "source": [
        "from openprompt import PromptForClassification\n",
        "promptModel = PromptForClassification(\n",
        "    template = promptTemplate,\n",
        "    plm = plm,\n",
        "    verbalizer = promptVerbalizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg9yZuMr0w1v"
      },
      "source": [
        "### **Step 6: Define a DataLoader**\n",
        "A PromptDataLoader is basically a prompt version of pytorch Dataloader, which also includes a Tokenizer, a Template and a TokenizerWrapper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NthXxZkq0w1v",
        "outputId": "af638806-2a2e-4a57-e49c-a97db78f5f71"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tokenizing: 2it [00:00, 1001.62it/s]\n"
          ]
        }
      ],
      "source": [
        "from openprompt import PromptDataLoader\n",
        "data_loader = PromptDataLoader(\n",
        "    dataset = dataset,\n",
        "    tokenizer = tokenizer,\n",
        "    template = promptTemplate,\n",
        "    tokenizer_wrapper_class=WrapperClass,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCzQ2pbO0w1v"
      },
      "source": [
        "### **Step 7: Train and inference**\n",
        "Done! We can conduct training and inference the same as other processes in Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Skt0WDDd0w1w",
        "outputId": "3eb9d2d1-2a22-41dd-d5c2-98ae48e1b6de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Albert Einstein was one of the greatest intellects of his time.\n",
            "Sentiment:  positive\n",
            "\n",
            "The film was badly made.\n",
            "Sentiment:  negative\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# making zero-shot inference using pretrained MLM with prompt\n",
        "promptModel.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "        logits = promptModel(batch)\n",
        "        preds = torch.argmax(logits, dim = -1)\n",
        "        print(next(x.text_a for x in dataset if x.guid==batch[\"guid\"]))\n",
        "        print(\"Sentiment: \", classes[preds])\n",
        "        print()\n",
        "# predictions would be 1, 0 for classes 'positive', 'negative'"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c536d464c425baa195bec6e8a0508c63c0f4224f917fb0d5c1fc3d571634ac5a"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}